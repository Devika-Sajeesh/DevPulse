# backend/services/ai_summary.py

import os, json, httpx
from dotenv import load_dotenv

load_dotenv()



GROQ_KEY = os.getenv("GROQ_API_KEY")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
MODEL = os.getenv("AI_MODEL", "gpt-4o-mini")

# --- Utility: truncate long text ---
def truncate(text: str, max_chars: int = 2000) -> str:
    if not text:
        return ""
    return text[:max_chars] + ("..." if len(text) > max_chars else "")

async def generate_ai_metrics(radon: str, cloc: str, pylint: str):
    """
    Generates structured AI metrics, including the AI Probability (w2).
    """
    if not (GROQ_KEY or OPENAI_KEY):
        return {"error": "No API key set. Please configure GROQ_API_KEY or OPENAI_API_KEY."}

    # New prompt to enforce structured JSON output with AI probability
    prompt = (
        "Analyze the following static analysis reports. Your primary task is to estimate "
        "the probability that this code was substantially generated by a Large Language Model (LLM/AI). "
        "Focus on patterns like excessive comments, token-like variable names, lack of idiomatic code, "
        "and common LLM verbosity in docstrings or function bodies. "
        "Secondly, provide actionable recommendations based on the combined static metrics.\n"
        "Output ONLY a single JSON object with the following schema:\n"
        '{\n'
        '  "ai_probability": <float from 0.0 to 1.0>,\n'
        '  "ai_risk_notes": "<concise text explanation for the probability score, max 10 words>",\n'
        '  "recommendations": [<list of 3 developer suggestions>]\n'
        '}\n\n'
        f"Radon (trimmed):\n{truncate(radon)}\n\n"
        f"CLOC (trimmed):\n{truncate(cloc)}\n\n"
        f"Pylint (trimmed):\n{truncate(pylint)}"
    )

    api_url = (
        "https://api.groq.com/openai/v1/chat/completions" if GROQ_KEY
        else "https://api.openai.com/v1/chat/completions"
    )
    headers = {
        "Authorization": f"Bearer {GROQ_KEY or OPENAI_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 600,
        "response_format": {"type": "json_object"} # Specify JSON output for reliable parsing
    }

    async with httpx.AsyncClient(timeout=30.0) as client:
        r = await client.post(api_url, json=payload, headers=headers)
        data = r.json()

    # Defensive parsing
    try:
        content = data["choices"][0]["message"]["content"]
    except Exception:
        return {"error": f"Bad response from AI: {data}"}

    # Extract and parse JSON safely
    try:
        # Since we request JSON, we attempt to load the content directly
        parsed_json = json.loads(content)
        # Ensure core fields are present, provide sensible defaults if missing
        return {
            "ai_probability": float(parsed_json.get("ai_probability", 0.0)),
            "ai_risk_notes": parsed_json.get("ai_risk_notes", "N/A"),
            "recommendations": parsed_json.get("recommendations", [])
        }
    except Exception:
        # Fallback if AI output is not perfect JSON
        return {"raw": content, "error": "Invalid JSON from AI", "ai_probability": 0.0}