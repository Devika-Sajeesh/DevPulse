"""
AI-powered code analysis summary service.

Provides AI-generated insights using Groq or OpenAI APIs with
comprehensive error handling, caching, and fallback strategies.
"""

import json
import hashlib
from typing import Dict, Any, Optional
import httpx
from backend.config import get_settings
from backend.utils.logger import setup_logger, log_execution_time
from backend.utils.exceptions import AIServiceError

logger = setup_logger(__name__)
settings = get_settings()

# Simple in-memory cache for AI responses
_ai_cache: Dict[str, Dict[str, Any]] = {}

def _truncate(text: str, max_chars: int = 2000) -> str:
    """
    Truncate text to maximum length.
    
    Args:
        text: Text to truncate
        max_chars: Maximum characters
    
    Returns:
        Truncated text
    """
    if not text:
        return ""
    return text[:max_chars] + ("..." if len(text) > max_chars else "")


def _generate_cache_key(radon: str, cloc: str, pylint: str) -> str:
    """
    Generate cache key for AI analysis.
    
    Args:
        radon: Radon output
        cloc: CLOC output
        pylint: Pylint output
    
    Returns:
        Cache key hash
    """
    content = f"{radon}|{cloc}|{pylint}"
    return hashlib.md5(content.encode()).hexdigest()

@log_execution_time(logger)
async def generate_ai_metrics(radon: str, cloc: str, pylint: str) -> Dict[str, Any]:
    """
    Generate structured AI metrics including AI code probability.
    
    Uses caching to avoid redundant API calls and implements
    comprehensive error handling with fallback strategies.
    
    Args:
        radon: Radon analysis output
        cloc: CLOC analysis output
        pylint: Pylint analysis output
    
    Returns:
        Dictionary containing:
            - ai_probability: Probability code was AI-generated (0-1)
            - ai_risk_notes: Explanation of the probability
            - recommendations: List of actionable recommendations
    
    Raises:
        AIServiceError: If AI service fails and no fallback available
    """
    # Check if AI service is configured
    if not settings.has_ai_service():
        logger.warning("No AI service configured, returning default metrics")
        return _get_fallback_metrics()
    
    # Check cache first
    cache_key = _generate_cache_key(radon, cloc, pylint)
    if cache_key in _ai_cache:
        logger.info("Returning cached AI metrics")
        return _ai_cache[cache_key]

    try:
        # Build prompt
        prompt = _build_analysis_prompt(radon, cloc, pylint)
        
        # Make API request
        result = await _call_ai_api(prompt)
        
        # Cache successful result
        _ai_cache[cache_key] = result
        
        # Limit cache size
        if len(_ai_cache) > 100:
            # Remove oldest entry
            _ai_cache.pop(next(iter(_ai_cache)))
        
        return result
        
    except Exception as e:
        logger.error(f"AI metrics generation failed: {e}", exc_info=True)
        # Return fallback metrics instead of failing
        return _get_fallback_metrics()


def _build_analysis_prompt(radon: str, cloc: str, pylint: str) -> str:
    """
    Build AI analysis prompt.
    
    Args:
        radon: Radon output
        cloc: CLOC output
        pylint: Pylint output
    
    Returns:
        Formatted prompt string
    """
    return (
        "Analyze the following static analysis reports. Your primary task is to estimate "
        "the probability that this code was substantially generated by a Large Language Model (LLM/AI). "
        "Focus on patterns like excessive comments, token-like variable names, lack of idiomatic code, "
        "and common LLM verbosity in docstrings or function bodies. "
        "Secondly, provide actionable recommendations based on the combined static metrics.\n"
        "Output ONLY a single JSON object with the following schema:\n"
        '{\n'
        '  "ai_probability": <float from 0.0 to 1.0>,\n'
        '  "ai_risk_notes": "<concise text explanation for the probability score, max 10 words>",\n'
        '  "recommendations": [<list of 3 developer suggestions>]\n'
        '}\n\n'
        f"Radon (trimmed):\n{_truncate(radon)}\n\n"
        f"CLOC (trimmed):\n{_truncate(cloc)}\n\n"
        f"Pylint (trimmed):\n{_truncate(pylint)}"
    )


async def _call_ai_api(prompt: str) -> Dict[str, Any]:
    """
    Call AI API with the analysis prompt.
    
    Args:
        prompt: Analysis prompt
    
    Returns:
        Parsed AI response
    
    Raises:
        AIServiceError: If API call fails
    """
    api_url = settings.get_ai_api_url()
    api_key = settings.get_ai_api_key()
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    
    payload = {
        "model": settings.ai_model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 600,
        "response_format": {"type": "json_object"}
    }
    
    try:
        async with httpx.AsyncClient(timeout=settings.ai_timeout) as client:
            logger.debug(f"Calling AI API: {api_url}")
            response = await client.post(api_url, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()
        
        # Extract content from response
        try:
            content = data["choices"][0]["message"]["content"]
        except (KeyError, IndexError) as e:
            raise AIServiceError(
                f"Invalid AI API response structure: {e}",
                service="AI",
                details={"response": str(data)[:500]}
            )
        
        # Parse JSON content
        try:
            parsed_json = json.loads(content)
            
            # Validate and extract fields
            result = {
                "ai_probability": float(parsed_json.get("ai_probability", 0.0)),
                "ai_risk_notes": parsed_json.get("ai_risk_notes", "N/A"),
                "recommendations": parsed_json.get("recommendations", [])
            }
            
            # Validate probability range
            result["ai_probability"] = max(0.0, min(1.0, result["ai_probability"]))
            
            logger.info(
                f"AI analysis complete: probability={result['ai_probability']:.2f}"
            )
            
            return result
            
        except json.JSONDecodeError as e:
            logger.warning(f"AI returned invalid JSON: {e}")
            raise AIServiceError(
                "AI service returned invalid JSON",
                service="AI",
                details={"content": content[:500]}
            )
    
    except httpx.HTTPStatusError as e:
        logger.error(f"AI API HTTP error: {e}")
        raise AIServiceError(
            f"AI API request failed: {e.response.status_code}",
            service="AI",
            details={"status_code": e.response.status_code}
        )
    except httpx.TimeoutException:
        logger.error("AI API request timed out")
        raise AIServiceError(
            "AI API request timed out",
            service="AI"
        )
    except Exception as e:
        logger.error(f"Unexpected AI API error: {e}", exc_info=True)
        raise AIServiceError(
            f"AI service error: {str(e)}",
            service="AI"
        )


def _get_fallback_metrics() -> Dict[str, Any]:
    """
    Get fallback metrics when AI service is unavailable.
    
    Returns:
        Default metrics dictionary
    """
    return {
        "ai_probability": 0.0,
        "ai_risk_notes": "AI analysis unavailable",
        "recommendations": [
            "Review code complexity and refactor high-complexity functions",
            "Improve code documentation and comments",
            "Add comprehensive unit tests for critical paths"
        ]
    }